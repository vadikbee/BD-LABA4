# sentiment_app.py

import streamlit as st
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from pymorphy3 import MorphAnalyzer
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import base64
import joblib 
import os 

# ----- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∑–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ -----

st.set_page_config(
    page_title="–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤",
    page_icon="üå∏",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ NLTK
@st.cache_resource
def download_nltk_resources():
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')
download_nltk_resources()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ base64
def get_base64(bin_file):
    with open(bin_file, 'rb') as f:
        data = f.read()
    return base64.b64encode(data).decode()

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —Ñ–æ–Ω–∞
def set_background(jpg_file):
    bin_str = get_base64(jpg_file)
    page_bg_img = f'''
    <style>
    .stApp {{
        background-image: url("data:image/jpg;base64,{bin_str}");
        background-size: cover;
    }}
    </style>
    '''
    st.markdown(page_bg_img, unsafe_allow_html=True)

try:
    set_background('sakura.jpg')
except FileNotFoundError:
    st.warning("–§–∞–π–ª 'sakura.jpg' –Ω–µ –Ω–∞–π–¥–µ–Ω. –§–æ–Ω –Ω–µ –±—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω.")

# –ö–∞—Å—Ç–æ–º–Ω—ã–π CSS –¥–ª—è —Ç–µ–º—ã "–ù–æ—á–Ω–∞—è —Å–∞–∫—É—Ä–∞"
st.markdown("""
<style>
    .stApp { color: #FFFFFF; }
    h1, h2, h3 { color: #FFC0CB; }
    .css-1d391kg {
        background-color: rgba(0, 0, 0, 0.6);
        border-right: 2px solid #FFC0CB;
    }
    .stButton>button {
        background-color: #FF69B4; color: #FFFFFF; border-radius: 8px; border: 1px solid #FFC0CB;
    }
    .stButton>button:hover {
        background-color: #FFC0CB; color: #000000; border: 1px solid #FF69B4;
    }
    .stTextArea textarea {
        background-color: rgba(30, 30, 30, 0.7); color: #FFFFFF; border: 1px solid #FFC0CB;
    }
    .stMetric {
        background-color: rgba(40, 40, 40, 0.7); border-radius: 10px; padding: 10px; border: 1px solid #FF69B4;
    }
    .stDataFrame { background-color: rgba(0, 0, 0, 0.5); }
</style>
""", unsafe_allow_html=True)

# –°—Ç–∏–ª—å –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ Matplotlib
matplotlib.rc('axes', facecolor='black', edgecolor='pink', labelcolor='white', titlecolor='pink')
matplotlib.rc('xtick', color='white'); matplotlib.rc('ytick', color='white')
matplotlib.rc('figure', facecolor='black', edgecolor='pink')
matplotlib.rc('legend', facecolor='black', edgecolor='pink', labelcolor='white')

# ----- –§—É–Ω–∫—Ü–∏–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è -----

# –ö—ç—à–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
@st.cache_resource
def load_model_and_vectorizer():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –∏–∑ —Ñ–∞–π–ª–æ–≤."""
    model_path = 'linear_svc_model.joblib'
    vectorizer_path = 'tfidf_vectorizer.joblib'
    
    if not os.path.exists(model_path) or not os.path.exists(vectorizer_path):
        st.error(
            "–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ `train_model.py`,"
            "—á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å."
        )
        st.stop()

    classifier = joblib.load(model_path)
    vectorizer = joblib.load(vectorizer_path)
    return classifier, vectorizer

# –ö—ç—à–∏—Ä—É–µ–º NLP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
@st.cache_resource
def get_nlp_tools():
    morph = MorphAnalyzer()
    russian_stopwords = stopwords.words("russian")
    return morph, russian_stopwords

morph, russian_stopwords = get_nlp_tools()

def preprocess_text(text):
    """–û—á–∏—â–∞–µ—Ç –∏ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç (—Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–¥–µ–Ω—Ç–∏—á–Ω–∞ —Ç–æ–π, —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)."""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[^–∞-—è–ê-–Ø—ë–Å\s]', ' ', text)
    words = text.split()
    lemmatized_words = [morph.parse(word)[0].normal_form for word in words if word not in russian_stopwords]
    return " ".join(lemmatized_words)

# --- –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è ---

st.title('üå∏ –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ —Ñ–∏–ª—å–º—ã')

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å—Ä–∞–∑—É, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
classifier, vectorizer = load_model_and_vectorizer()
st.session_state['model_loaded'] = True

st.sidebar.header("–ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
app_mode = st.sidebar.selectbox(
    "–í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª:",
    ["–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏", "–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏"]
)

if app_mode == "–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏":
    st.header("–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ—Ç–∑—ã–≤–∞")
    
    review_text = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞ –∑–¥–µ—Å—å:", height=200, 
                               placeholder="–°–µ–≥–æ–¥–Ω—è –ø–æ—Å–º–æ—Ç—Ä–µ–ª –Ω–æ–≤—ã–π —Ñ–∏–ª—å–º, –∏ –æ–Ω –æ–∫–∞–∑–∞–ª—Å—è –ø—Ä–æ—Å—Ç–æ –≤–æ—Å—Ö–∏—Ç–∏—Ç–µ–ª—å–Ω—ã–º!...")

    if st.button("–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"):
        if review_text.strip() == "":
            st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞.")
        else:
            with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é..."):
                processed_text = preprocess_text(review_text)
                vectorized_text = vectorizer.transform([processed_text])
                prediction = classifier.predict(vectorized_text)[0]
                
                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞:")
                if prediction == 'pos':
                    st.success(f"## –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π üëç")
                elif prediction == 'neu':
                    st.warning(f"## –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π üòê")
                else:
                    st.error(f"## –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π üëé")

elif app_mode == "–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏":
    st.header("–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏")
    st.info("–ó–¥–µ—Å—å –ø–æ–∫–∞–∑–∞–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö `kp_valid.csv`, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.")

    try:
        df_valid = pd.read_csv('kp_valid.csv', sep='|').dropna().drop_duplicates()
        
        with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –æ—Ü–µ–Ω–∫–∞..."):
            df_valid['processed_review'] = df_valid['review'].apply(preprocess_text)
            X_valid = vectorizer.transform(df_valid['processed_review'])
            y_valid = df_valid['sentiment']

            y_pred_valid = classifier.predict(X_valid)
            valid_accuracy = accuracy_score(y_valid, y_pred_valid)
            report = classification_report(y_valid, y_pred_valid, zero_division=0)
            cm = confusion_matrix(y_valid, y_pred_valid, labels=classifier.classes_)

        st.metric("–¢–æ—á–Ω–æ—Å—Ç—å (Accuracy) –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–º –Ω–∞–±–æ—Ä–µ", f"{valid_accuracy:.2%}")
        
        st.text("–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç (Classification Report):")
        st.code(report)
        
        st.subheader("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (Confusion Matrix)")
        fig, ax = plt.subplots(figsize=(6, 4))
        sns.heatmap(cm, annot=True, fmt='d', cmap='RdPu', 
                    xticklabels=classifier.classes_, yticklabels=classifier.classes_, ax=ax)
        ax.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        ax.set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        st.pyplot(fig)

    except FileNotFoundError:
        st.error("–§–∞–π–ª 'kp_valid.csv' –Ω–µ –Ω–∞–π–¥–µ–Ω. –û–Ω –Ω—É–∂–µ–Ω –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏.")